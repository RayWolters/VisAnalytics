{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "# PRE PROCESSING\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# TSNE - TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIP INSTALLs\n",
    "# !pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS METHODS \n",
    "\n",
    "# SORT THE ARTICLES in folder\n",
    "def sortedAlphanumeric(path):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(path, key=alphanum_key)\n",
    "\n",
    "# LOWERCASE\n",
    "def lowercase(file):\n",
    "    return file.lower()\n",
    "\n",
    "# REMOVE punctuation\n",
    "def removePunctuation(file):\n",
    "    return file.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "# REMOVE WHITE SPACE\n",
    "def removeSpace(file):\n",
    "    return \" \".join(file.split())\n",
    "\n",
    "# REMOVE STOPWORDS\n",
    "def removeStopwords(file):\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    wordTokens = word_tokenize(file)\n",
    "    l = [w for w in word_tokenize(file) if w not in set(stopwords.words(\"english\"))]\n",
    "    l = ' '.join(l)\n",
    "    return l\n",
    "\n",
    "# LEMMATIZATION to root form but still remaining a valid word in English instead of stemming (yields invalid words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(file):\n",
    "    wordTokens = word_tokenize(file)\n",
    "    l = [lemmatizer.lemmatize(w, pos='v') for w in wordTokens]\n",
    "    l = ' '.join(l)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ALL DOCUMENTS\n",
    "article_path = '../data/articles/'\n",
    "article_dirs = sortedAlphanumeric(os.listdir(article_path))\n",
    "articles = [open(article_path + article).read() for article in article_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESS\n",
    "# LOWERCASE\n",
    "articles2 = []\n",
    "for article in articles:\n",
    "    articles2.append(lowercase(article))\n",
    "\n",
    "# NO PUNCT\n",
    "articles3 = []\n",
    "for article in articles2:\n",
    "    articles3.append(removePunctuation(article))\n",
    "\n",
    "# REMOVE SPACE\n",
    "articles4 = []\n",
    "for article in articles3:\n",
    "    articles4.append(removeSpace(article))\n",
    "\n",
    "# REMOVE STOPWORDS\n",
    "articles5 = []\n",
    "for article in articles4:\n",
    "    articles5.append(removeStopwords(article))\n",
    "\n",
    "# TRANSFORM EACH WORD TO ROOT\n",
    "articles6 = []\n",
    "for article in articles5:\n",
    "    articles6.append(lemmatization(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TFIDF to vectorize the text\n",
    "# https://www.scikit-yb.org/en/latest/api/text/tsne.html\n",
    "# https://buhrmann.github.io/tfidf-analysis.html\n",
    "# https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "Path(\"tf_idf_output\").mkdir(parents=True, exist_ok=True)\n",
    "output_filenames = [str(txt_file).replace(\".txt\", \".csv\").replace(\"txt/\", \"tf_idf_output/\") for txt_file in article_dirs]\n",
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)\n",
    "transformedDocs = vectorizer.fit_transform(articles6).toarray()\n",
    "\n",
    "# for c, d in enumerate(transformedDocs):\n",
    "#     tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), d))\n",
    "    # one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "    # one_doc_as_df.to_csv(output_filenames[c])\n",
    "\n",
    "# list(zip(vectorizer.get_feature_names_out(), transformedDocs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "for article in articles6:\n",
    "    z.append(article.split())\n",
    "\n",
    "uniqueWords = {x for l in z for x in l}\n",
    "# https://beckernick.github.io/law-clustering/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d07dd9b2aa3880ac3b0b74fe20703b02e1aa89fb3940e6552b6960b956ac8a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
